{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to try to finetune the part of speech tagging pipeline in Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorForTokenClassification,\\\n",
    "AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/valentine/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/valentine/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Brown corpus\n",
    "corpus = brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sample from the corpus\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tags and words\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for sentence_tag_pairs in corpus:\n",
    "    tokens = []\n",
    "    target = []\n",
    "    for token, tag in sentence_tag_pairs:\n",
    "        tokens.append(token)\n",
    "        target.append(tag)\n",
    "    inputs.append(tokens)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to json\n",
    "with open('data.json', 'w') as f:\n",
    "    for x, y in zip(inputs, targets):\n",
    "        j = {'inputs': x, 'targets': y}\n",
    "        s = json.dumps(j)\n",
    "        f.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-88f9b318b5682e5c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/valentine/.cache/huggingface/datasets/json/default-88f9b318b5682e5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ee47d467864ef08957f0fc494e987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81536b34e3b49aaa3fcbb4f6a6713db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/valentine/.cache/huggingface/datasets/json/default-88f9b318b5682e5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61b16b89cc84bd381e11b39c67240dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the dataset\n",
    "data = load_dataset('json', data_files='data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 57340\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset structure\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'targets'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a sample from the dataset\n",
    "small = data['train'].shuffle().select(range(20000))\n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train and test sets\n",
    "data = small.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': ['She',\n",
       "  'had',\n",
       "  'a',\n",
       "  'dried-out',\n",
       "  'quality',\n",
       "  '--',\n",
       "  'a',\n",
       "  'gray',\n",
       "  ',',\n",
       "  'lean',\n",
       "  'woman',\n",
       "  ',',\n",
       "  'not',\n",
       "  'unattractive',\n",
       "  '.'],\n",
       " 'targets': ['PRON',\n",
       "  'VERB',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  '.',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'ADV',\n",
       "  'ADJ',\n",
       "  '.']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'targets': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the set of tags\n",
    "target_set = set()\n",
    "for target in targets:\n",
    "    target_set = target_set.union(target)\n",
    "target_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map ids to labels and labels to ids\n",
    "target_list = list(target_set)\n",
    "id2label = {k: v for k, v in enumerate(target_list)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a checkpoint and a tokenizer\n",
    "checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1153, 1125, 170, 9490, 118, 1149, 3068, 118, 118, 170, 5021, 117, 8290, 1590, 117, 1136, 8362, 19934, 19366, 3946, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize a sample sentence\n",
    "idx = 0\n",
    "t = tokenizer(data['train'][idx]['inputs'], is_split_into_words=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'She',\n",
       " 'had',\n",
       " 'a',\n",
       " 'dried',\n",
       " '-',\n",
       " 'out',\n",
       " 'quality',\n",
       " '-',\n",
       " '-',\n",
       " 'a',\n",
       " 'gray',\n",
       " ',',\n",
       " 'lean',\n",
       " 'woman',\n",
       " ',',\n",
       " 'not',\n",
       " 'un',\n",
       " '##att',\n",
       " '##rac',\n",
       " '##tive',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to align labels\n",
    "def align_targets(labels, word_ids):\n",
    "    aligned_labels = []\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            label = -100\n",
    "        else:\n",
    "            label = label2id[labels[word]]\n",
    "        aligned_labels.append(label)\n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 4,\n",
       " 11,\n",
       " 4,\n",
       " 10,\n",
       " 11,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " -100]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data['train'][idx]['targets']\n",
    "word_ids = t.word_ids()\n",
    "aligned_targets = align_targets(labels, word_ids)\n",
    "aligned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "She\tPRON\n",
      "had\tVERB\n",
      "a\tDET\n",
      "dried\tADJ\n",
      "-\tADJ\n",
      "out\tADJ\n",
      "quality\tNOUN\n",
      "-\t.\n",
      "-\t.\n",
      "a\tDET\n",
      "gray\tADJ\n",
      ",\t.\n",
      "lean\tADJ\n",
      "woman\tNOUN\n",
      ",\t.\n",
      "not\tADV\n",
      "un\tADJ\n",
      "##att\tADJ\n",
      "##rac\tADJ\n",
      "##tive\tADJ\n",
      ".\t.\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = [id2label[i] if i >= 0 else None for i in aligned_targets]\n",
    "for x, y in zip(t.tokens(), aligned_labels):\n",
    "    print(x, y, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to tokenize data batches\n",
    "def tokenize_fn(batch):\n",
    "    tokenized_inputs = tokenizer(batch['inputs'], truncation=True, is_split_into_words=True)\n",
    "    labels_batch = batch['targets']\n",
    "    aligned_labels_batch = []\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3858900bd5c4f35ba45b8a4a645b66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955b1ed94f0a4054bcd38d9babfd9e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets = data.map(tokenize_fn, batched=True, remove_columns=data['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return [val for sublist in list_of_lists for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to compute evaluation metrics\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    labels_jagged = [[t for t in label if t != -100] for label in labels]\n",
    "    pred_jagged = [[p for p, t in zip(ps, ts) if t != -100] for ps, ts in zip(predictions, labels)]\n",
    "    labels_flat = flatten(labels_jagged)\n",
    "    predictions_flat = flatten(pred_jagged)\n",
    "    acc = accuracy_score(labels_flat, predictions_flat)\n",
    "    f1 = f1_score(labels_flat, predictions_flat, average='macro')\n",
    "    return {'f1': f1, 'accuracy': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6, 'accuracy': 0.8}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function on dummy data\n",
    "labels = [[-100, 0, 0, 1, 2, 1, -100]]\n",
    "logits = np.array([[\n",
    "  [0.8, 0.1, 0.1],\n",
    "  [0.8, 0.1, 0.1],\n",
    "  [0.8, 0.1, 0.1],\n",
    "  [0.1, 0.8, 0.1],\n",
    "  [0.1, 0.8, 0.1],\n",
    "  [0.1, 0.8, 0.1],\n",
    "  [0.1, 0.8, 0.1],\n",
    "]])\n",
    "compute_metrics((logits, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# use the model with our checkpoint\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "training_args = TrainingArguments('distilbert-finetuning-ner',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  save_strategy='epoch',\n",
    "                                  num_train_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentine/opt/anaconda3/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3750\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 49:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.053155</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.984382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>0.960914</td>\n",
       "      <td>0.985831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-finetuning-ner/checkpoint-1875\n",
      "Configuration saved in distilbert-finetuning-ner/checkpoint-1875/config.json\n",
      "Model weights saved in distilbert-finetuning-ner/checkpoint-1875/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-finetuning-ner/checkpoint-1875/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-finetuning-ner/checkpoint-1875/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-finetuning-ner/checkpoint-3750\n",
      "Configuration saved in distilbert-finetuning-ner/checkpoint-3750/config.json\n",
      "Model weights saved in distilbert-finetuning-ner/checkpoint-3750/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-finetuning-ner/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-finetuning-ner/checkpoint-3750/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.06804031995137533, metrics={'train_runtime': 2962.5117, 'train_samples_per_second': 10.127, 'train_steps_per_second': 1.266, 'total_flos': 383942507032512.0, 'train_loss': 0.06804031995137533, 'epoch': 2.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the trainer and train the model\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tokenized_datasets['train'],\n",
    "                  eval_dataset=tokenized_datasets['test'],\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to pos_model\n",
      "Configuration saved in pos_model/config.json\n",
      "Model weights saved in pos_model/pytorch_model.bin\n",
      "tokenizer config file saved in pos_model/tokenizer_config.json\n",
      "Special tokens file saved in pos_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "trainer.save_model('pos_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pos_model/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"pos_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"VERB\",\n",
      "    \"1\": \"PRT\",\n",
      "    \"2\": \"ADV\",\n",
      "    \"3\": \"NUM\",\n",
      "    \"4\": \"ADJ\",\n",
      "    \"5\": \"PRON\",\n",
      "    \"6\": \"X\",\n",
      "    \"7\": \"ADP\",\n",
      "    \"8\": \"CONJ\",\n",
      "    \"9\": \"DET\",\n",
      "    \"10\": \"NOUN\",\n",
      "    \"11\": \".\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \".\": 11,\n",
      "    \"ADJ\": 4,\n",
      "    \"ADP\": 7,\n",
      "    \"ADV\": 2,\n",
      "    \"CONJ\": 8,\n",
      "    \"DET\": 9,\n",
      "    \"NOUN\": 10,\n",
      "    \"NUM\": 3,\n",
      "    \"PRON\": 5,\n",
      "    \"PRT\": 1,\n",
      "    \"VERB\": 0,\n",
      "    \"X\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file pos_model/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"pos_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"VERB\",\n",
      "    \"1\": \"PRT\",\n",
      "    \"2\": \"ADV\",\n",
      "    \"3\": \"NUM\",\n",
      "    \"4\": \"ADJ\",\n",
      "    \"5\": \"PRON\",\n",
      "    \"6\": \"X\",\n",
      "    \"7\": \"ADP\",\n",
      "    \"8\": \"CONJ\",\n",
      "    \"9\": \"DET\",\n",
      "    \"10\": \"NOUN\",\n",
      "    \"11\": \".\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \".\": 11,\n",
      "    \"ADJ\": 4,\n",
      "    \"ADP\": 7,\n",
      "    \"ADV\": 2,\n",
      "    \"CONJ\": 8,\n",
      "    \"DET\": 9,\n",
      "    \"NOUN\": 10,\n",
      "    \"NUM\": 3,\n",
      "    \"PRON\": 5,\n",
      "    \"PRT\": 1,\n",
      "    \"VERB\": 0,\n",
      "    \"X\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pos_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at pos_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# use the pipeline with our model\n",
    "pos_tagger = pipeline('token-classification', model='pos_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'NOUN',\n",
       "  'score': 0.99970156,\n",
       "  'index': 1,\n",
       "  'word': 'Bill',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99974734,\n",
       "  'index': 2,\n",
       "  'word': 'Gates',\n",
       "  'start': 5,\n",
       "  'end': 10},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.99977285,\n",
       "  'index': 3,\n",
       "  'word': 'was',\n",
       "  'start': 11,\n",
       "  'end': 14},\n",
       " {'entity': 'DET',\n",
       "  'score': 0.99989676,\n",
       "  'index': 4,\n",
       "  'word': 'the',\n",
       "  'start': 15,\n",
       "  'end': 18},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99971145,\n",
       "  'index': 5,\n",
       "  'word': 'CEO',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.99987173,\n",
       "  'index': 6,\n",
       "  'word': 'of',\n",
       "  'start': 23,\n",
       "  'end': 25},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99975616,\n",
       "  'index': 7,\n",
       "  'word': 'Microsoft',\n",
       "  'start': 26,\n",
       "  'end': 35},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.99984086,\n",
       "  'index': 8,\n",
       "  'word': 'in',\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99981636,\n",
       "  'index': 9,\n",
       "  'word': 'Seattle',\n",
       "  'start': 39,\n",
       "  'end': 46},\n",
       " {'entity': '.',\n",
       "  'score': 0.9998752,\n",
       "  'index': 10,\n",
       "  'word': ',',\n",
       "  'start': 46,\n",
       "  'end': 47},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9997551,\n",
       "  'index': 11,\n",
       "  'word': 'Washington',\n",
       "  'start': 48,\n",
       "  'end': 58},\n",
       " {'entity': '.',\n",
       "  'score': 0.99984944,\n",
       "  'index': 12,\n",
       "  'word': '.',\n",
       "  'start': 58,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the pipeline on a sentence\n",
    "pos_tagger('Bill Gates was the CEO of Microsoft in Seattle, Washington.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
