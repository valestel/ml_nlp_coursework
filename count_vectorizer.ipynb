{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to use CountVectorizer from Scikit-learn with various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/valentine/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/valentine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/valentine/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download BBC text classification dataset\n",
    "# original dataset on Kaggle: https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification)\n",
    "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset in Pandas dataframe\n",
    "df = pd.read_csv('bbc_text_cls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Japan narrowly escapes recession\\n\\nJapan's ec...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jobs growth still slow in the US\\n\\nThe US cre...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>India calls for fair trade rules\\n\\nIndia, whi...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ethiopia's crop production up 24%\\n\\nEthiopia ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Court rejects $280bn tobacco case\\n\\nA US gove...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
       "5  Japan narrowly escapes recession\\n\\nJapan's ec...  business\n",
       "6  Jobs growth still slow in the US\\n\\nThe US cre...  business\n",
       "7  India calls for fair trade rules\\n\\nIndia, whi...  business\n",
       "8  Ethiopia's crop production up 24%\\n\\nEthiopia ...  business\n",
       "9  Court rejects $280bn tobacco case\\n\\nA US gove...  business"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save texts and labels\n",
    "inputs = df['text']\n",
    "labels = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb0290f1710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check labels distribution: they don't seem too imbalanced\n",
    "labels.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train and test sets\n",
    "inputs_train, inputs_test, Ytrain, Ytest = train_test_split(inputs, labels, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speech mapping function for a custom tokenizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    mapping = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return mapping.get(treebank_tag[0], wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom tokenizer based on lemmatization\n",
    "class LemmaTokenizer():\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        words_and_tags = nltk.pos_tag(tokens)\n",
    "        return [self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in words_and_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom tokenizer based on stemming\n",
    "class StemTokenizer():\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        return [self.porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of count vectorizers\n",
    "vectorizers = {'default': CountVectorizer(),\n",
    "               'with stopwords': CountVectorizer(stop_words='english'),\n",
    "               'with lemmatization tokenizer': CountVectorizer(tokenizer=LemmaTokenizer()),\n",
    "               'with stemming tokenizer': CountVectorizer(tokenizer=StemTokenizer()),\n",
    "               'with splitting tokenizer': CountVectorizer(tokenizer=lambda s: s.split())\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model and lists to keep the vectorizers types and scores\n",
    "model = MultinomialNB()\n",
    "vectorizers_names = []\n",
    "time_scores = []\n",
    "train_scores = []\n",
    "test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to use the vectorizer, fit the model and save the scores \n",
    "def count_vectorizer(v):\n",
    "    vectorizer = v\n",
    "    Xtrain = vectorizer.fit_transform(inputs_train)\n",
    "    Xtest = vectorizer.transform(inputs_test)\n",
    "    t0 = datetime.now()\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    dt = (datetime.now() - t0).total_seconds()\n",
    "    train_score = model.score(Xtrain, Ytrain)\n",
    "    test_score = model.score(Xtest, Ytest)\n",
    "    time_scores.append(dt)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to each vectorizer\n",
    "for k, v in vectorizers.items():\n",
    "    vectorizers_names.append(k)\n",
    "    count_vectorizer(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a summary table\n",
    "summary = pd.DataFrame({'time': time_scores, 'train score': train_scores, 'test score': test_scores},\n",
    "                       index=vectorizers_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>0.008768</td>\n",
       "      <td>0.992206</td>\n",
       "      <td>0.971275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with stopwords</th>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.992806</td>\n",
       "      <td>0.976661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with lemmatization tokenizer</th>\n",
       "      <td>0.006932</td>\n",
       "      <td>0.992206</td>\n",
       "      <td>0.967684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with stemming tokenizer</th>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.989209</td>\n",
       "      <td>0.969479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with splitting tokenizer</th>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.995204</td>\n",
       "      <td>0.971275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  time  train score  test score\n",
       "default                       0.008768     0.992206    0.971275\n",
       "with stopwords                0.007166     0.992806    0.976661\n",
       "with lemmatization tokenizer  0.006932     0.992206    0.967684\n",
       "with stemming tokenizer       0.006761     0.989209    0.969479\n",
       "with splitting tokenizer      0.007776     0.995204    0.971275"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary\n",
    "# The count vectorizer with stopwords seems to have the best test score\n",
    "# The vectorizers using linguistic tokenizers (lemmatization, stemming) have slightly worse test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
