{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to try to finetune the question answering pipeline in Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/valentine/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851fdaf8b1a144d4b174cd9e4861ad1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Stanford Question Answering Dataset\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'University_of_Notre_Dame'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a sample\n",
    "raw_datasets[\"train\"][1][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is in front of the Notre Dame Main Building?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['a copper statue of Christ'], 'answer_start': [188]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a checkpoint and a tokenizer\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to find the index of an answer token\n",
    "def find_answer_token_idx(ctx_start, ctx_end, ans_start_char, ans_end_char, offset):\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "        # the target is (0, 0), nothing to do\n",
    "        pass\n",
    "    else:\n",
    "        # find the start and end token positions\n",
    "        i = ctx_start\n",
    "        for start_end_char in offset[ctx_start:]:\n",
    "            start, end = start_end_char\n",
    "            if start == ans_start_char:\n",
    "                start_idx = i\n",
    "            if end == ans_end_char:\n",
    "                end_idx = i\n",
    "                break\n",
    "            i += 1\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "# create a function to tokenize the train set\n",
    "def tokenize_fn_train(batch):\n",
    "    # remove extra whitespaces\n",
    "    questions = [q.strip() for q in batch['question']]\n",
    "    # tokenize the data with padding\n",
    "    inputs = tokenizer(questions,\n",
    "                       batch['context'],\n",
    "                       max_length=max_length,\n",
    "                       truncation='only_second',\n",
    "                       stride=stride,\n",
    "                       return_overflowing_tokens=True,\n",
    "                       return_offsets_mapping=True,\n",
    "                       padding='max_length')\n",
    "    \n",
    "    offset_mapping = inputs.pop('offset_mapping')\n",
    "    orig_sample_idxs = inputs.pop('overflow_to_sample_mapping')\n",
    "    answers = batch['answers']\n",
    "    start_idxs, end_idxs = [], []\n",
    "    \n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = orig_sample_idxs[i]\n",
    "        answer = answers[sample_idx]\n",
    "        answer_start_char = answer['answer_start'][0]\n",
    "        answer_end_char = answer_start_char + len(answer['text'][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # find start + end of context (first 1 and last 1)\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1   \n",
    "        start_idx, end_idx = find_answer_token_idx(context_start,\n",
    "                                                   context_end,\n",
    "                                                   answer_start_char,\n",
    "                                                   answer_end_char,\n",
    "                                                   offset)\n",
    "        start_idxs.append(start_idx)\n",
    "        end_idxs.append(end_idx)\n",
    "    \n",
    "    inputs['start_positions'] = start_idxs\n",
    "    inputs['end_positions'] = end_idxs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540f1b7b87d54f34b9b26af90edd21ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(87599, 88729)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(tokenize_fn_train,\n",
    "                                          batched=True,\n",
    "                                          remove_columns=raw_datasets[\"train\"].column_names)\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to tokenize the validation set\n",
    "# the targets are not needed since they will be compared with the original answer\n",
    "def tokenize_fn_validation(batch):\n",
    "    # remove extra whitespaces\n",
    "    questions = [q.strip() for q in batch['question']]\n",
    "    # tokenize the data with padding\n",
    "    inputs = tokenizer(questions,\n",
    "                       batch['context'],\n",
    "                       max_length=max_length,\n",
    "                       truncation='only_second',\n",
    "                       stride=stride,\n",
    "                       return_overflowing_tokens=True,\n",
    "                       return_offsets_mapping=True,\n",
    "                       padding='max_length')\n",
    "    \n",
    "    orig_sample_idxs = inputs.pop('overflow_to_sample_mapping')\n",
    "    sample_ids = []\n",
    "    # rewrite offset mapping by replacing question tuples with None\n",
    "    for i in range(len(inputs['input_ids'])):\n",
    "        sample_idx = orig_sample_idxs[i]\n",
    "        sample_ids.append(batch['id'][sample_idx])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs['offset_mapping'][i]\n",
    "        inputs['offset_mapping'][i] = [x if sequence_ids[j] == 1 else None for j, x in enumerate(offset)]\n",
    "        \n",
    "    inputs['sample_id'] = sample_ids    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cdfd9bce7a412eb5dd62a253d32b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(tokenize_fn_validation,\n",
    "                                                    batched=True,\n",
    "                                                    remove_columns=raw_datasets[\"validation\"].column_names)\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentine/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# load evaluation metrics\n",
    "metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_largest = 20\n",
    "max_answer_length = 30\n",
    "# create a function to compute evaluation metrics\n",
    "def compute_metrics(start_logits, end_logits, processed_dataset, orig_dataset):\n",
    "    sample_id2idxs = {}\n",
    "    for i, id_ in enumerate(processed_dataset['sample_id']):\n",
    "        if id_ not in sample_id2idxs:\n",
    "            sample_id2idxs[id_] = [i]\n",
    "        else:\n",
    "            sample_id2idxs[id_].append(i)\n",
    "            \n",
    "    predicted_answers = []\n",
    "    for sample in tqdm(orig_dataset):\n",
    "        sample_id = sample['id']\n",
    "        context = sample['context']\n",
    "        # update the scores when looping through candidate answers\n",
    "        best_score = float('-inf')\n",
    "        best_answer = None\n",
    "        # loop through the expanded input samples (fixed size context windows)\n",
    "        # pick the highest probability start/end combination\n",
    "        for idx in sample_id2idxs[sample_id]:\n",
    "            start_logit = start_logits[idx]\n",
    "            end_logit = end_logits[idx]\n",
    "            offsets = processed_dataset[idx]['offset_mapping']\n",
    "            start_indices = (-start_logit).argsort()\n",
    "            end_indices = (-end_logit).argsort()\n",
    "            \n",
    "            for start_idx in start_indices[:n_largest]:\n",
    "                for end_idx in end_indices[n_largest:]:\n",
    "                    # skip answers not contained in context window\n",
    "                    if offsets[start_idx] is None or offsets[end_idx] is None:\n",
    "                        continue\n",
    "                    # skip answers where end < start\n",
    "                    if end_idx < start_idx:\n",
    "                        continue\n",
    "                    # skip answers that are too long\n",
    "                    if end_idx - start_idx + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    score = start_logit[start_idx] + end_logit[end_idx]\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        # find positions of start and end characters\n",
    "                        first_ch = offsets[start_idx][0]\n",
    "                        last_ch = offsets[end_idx][1]\n",
    "                        best_answer = context[first_ch:last_ch]\n",
    "        # save best answer\n",
    "        predicted_answers.append({'id': sample_id, 'prediction_text': best_answer})\n",
    "    # compute the metrics\n",
    "    true_answers = [{'id': x['id'], 'answers': x['answers']} for x in orig_dataset]\n",
    "    \n",
    "    return metric.compute(predictions=predicted_answers, references=true_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# use the model with our checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "args = TrainingArguments(\"finetuned-squad\",\n",
    "                         evaluation_strategy=\"no\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         learning_rate=2e-5,\n",
    "                         num_train_epochs=3,\n",
    "                         weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentine/opt/anaconda3/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 23:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to finetuned-squad/checkpoint-125\n",
      "Configuration saved in finetuned-squad/checkpoint-125/config.json\n",
      "Model weights saved in finetuned-squad/checkpoint-125/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned-squad/checkpoint-125/tokenizer_config.json\n",
      "Special tokens file saved in finetuned-squad/checkpoint-125/special_tokens_map.json\n",
      "Saving model checkpoint to finetuned-squad/checkpoint-250\n",
      "Configuration saved in finetuned-squad/checkpoint-250/config.json\n",
      "Model weights saved in finetuned-squad/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned-squad/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in finetuned-squad/checkpoint-250/special_tokens_map.json\n",
      "Saving model checkpoint to finetuned-squad/checkpoint-375\n",
      "Configuration saved in finetuned-squad/checkpoint-375/config.json\n",
      "Model weights saved in finetuned-squad/checkpoint-375/pytorch_model.bin\n",
      "tokenizer config file saved in finetuned-squad/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in finetuned-squad/checkpoint-375/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=3.6120107421875, metrics={'train_runtime': 1384.6235, 'train_samples_per_second': 2.167, 'train_steps_per_second': 0.271, 'total_flos': 293969475072000.0, 'train_loss': 3.6120107421875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the trainer and train the model\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=train_dataset.shuffle().select(range(1000)),\n",
    "                  eval_dataset=validation_dataset,\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: offset_mapping, sample_id. If offset_mapping, sample_id are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10822\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make predictions and evaluate\n",
    "trainer_output = trainer.predict(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(array([[-0.42320642, -4.2525625 , -3.7858965 , ..., -6.6363916 ,\n",
       "        -6.6032457 , -6.598208  ],\n",
       "       [-0.4200905 , -4.2481155 , -3.756441  , ..., -6.6373706 ,\n",
       "        -6.605947  , -6.5995436 ],\n",
       "       [-0.42662624, -4.176383  , -5.122679  , ..., -6.5414276 ,\n",
       "        -6.5389147 , -6.5705695 ],\n",
       "       ...,\n",
       "       [-0.5383989 , -4.5240765 , -5.186863  , ..., -6.5090423 ,\n",
       "        -6.502405  , -6.480877  ],\n",
       "       [-0.5170208 , -4.560294  , -5.08591   , ..., -6.5650644 ,\n",
       "        -6.497587  , -6.523964  ],\n",
       "       [-0.4799257 , -4.4987035 , -5.224111  , ..., -6.5554504 ,\n",
       "        -6.535227  , -6.511924  ]], dtype=float32), array([[-0.60021526, -4.3629136 , -4.563427  , ..., -5.8111224 ,\n",
       "        -5.834797  , -5.8093867 ],\n",
       "       [-0.6049708 , -4.36707   , -4.5640197 , ..., -5.8075447 ,\n",
       "        -5.830118  , -5.805607  ],\n",
       "       [-0.5852223 , -4.305908  , -4.573474  , ..., -5.8769684 ,\n",
       "        -5.8873005 , -5.811081  ],\n",
       "       ...,\n",
       "       [-0.6049764 , -4.4468246 , -4.8170404 , ..., -5.7235303 ,\n",
       "        -5.731379  , -5.7595367 ],\n",
       "       [-0.54139066, -4.853022  , -4.799987  , ..., -5.838605  ,\n",
       "        -5.941598  , -5.9018435 ],\n",
       "       [-0.58579195, -4.5268126 , -5.0191145 , ..., -5.753068  ,\n",
       "        -5.762236  , -5.793327  ]], dtype=float32)), label_ids=None, metrics={'test_runtime': 1492.3486, 'test_samples_per_second': 7.252, 'test_steps_per_second': 0.907})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, _, _ = trainer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42320642, -4.2525625 , -3.7858965 , ..., -6.6363916 ,\n",
       "         -6.6032457 , -6.598208  ],\n",
       "        [-0.4200905 , -4.2481155 , -3.756441  , ..., -6.6373706 ,\n",
       "         -6.605947  , -6.5995436 ],\n",
       "        [-0.42662624, -4.176383  , -5.122679  , ..., -6.5414276 ,\n",
       "         -6.5389147 , -6.5705695 ],\n",
       "        ...,\n",
       "        [-0.5383989 , -4.5240765 , -5.186863  , ..., -6.5090423 ,\n",
       "         -6.502405  , -6.480877  ],\n",
       "        [-0.5170208 , -4.560294  , -5.08591   , ..., -6.5650644 ,\n",
       "         -6.497587  , -6.523964  ],\n",
       "        [-0.4799257 , -4.4987035 , -5.224111  , ..., -6.5554504 ,\n",
       "         -6.535227  , -6.511924  ]], dtype=float32),\n",
       " array([[-0.60021526, -4.3629136 , -4.563427  , ..., -5.8111224 ,\n",
       "         -5.834797  , -5.8093867 ],\n",
       "        [-0.6049708 , -4.36707   , -4.5640197 , ..., -5.8075447 ,\n",
       "         -5.830118  , -5.805607  ],\n",
       "        [-0.5852223 , -4.305908  , -4.573474  , ..., -5.8769684 ,\n",
       "         -5.8873005 , -5.811081  ],\n",
       "        ...,\n",
       "        [-0.6049764 , -4.4468246 , -4.8170404 , ..., -5.7235303 ,\n",
       "         -5.731379  , -5.7595367 ],\n",
       "        [-0.54139066, -4.853022  , -4.799987  , ..., -5.838605  ,\n",
       "         -5.941598  , -5.9018435 ],\n",
       "        [-0.58579195, -4.5268126 , -5.0191145 , ..., -5.753068  ,\n",
       "         -5.762236  , -5.793327  ]], dtype=float32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits, end_logits = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769493d62cd949cba70dfc4490805d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.6433301797540208, 'f1': 12.698477764102211}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to qa_model\n",
      "Configuration saved in qa_model/config.json\n",
      "Model weights saved in qa_model/pytorch_model.bin\n",
      "tokenizer config file saved in qa_model/tokenizer_config.json\n",
      "Special tokens file saved in qa_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "trainer.save_model('qa_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file qa_model/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"qa_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file qa_model/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"qa_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file qa_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at qa_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# use the pipeline with our model\n",
    "qa = pipeline(\"question-answering\", model='qa_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.04340851679444313, 'start': 38, 'end': 45, 'answer': 'cookies'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the pipeline\n",
    "context = \"Today I went to the store to get some cookies.\"\n",
    "question = \"What did I buy?\"\n",
    "\n",
    "qa(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
